DL is a subset of ML

what are 3 primary domains in which DL used/researched?
CV, NLP, speech recognition



ML - not explicitly programmed
ML algorithms observe a pattern and attempt to directly or indirectly imitate the task

What do directly or indirectly  correspond to?
supervised and nonsupervised learning

supervised - the direct imitation of a pattern between two datasets
take input dataset and output output dataset

transforming what you know into what you want to know



machine learning has
supervised and unsupervised
parametric and nonparametric

supervised learning transforms datasets
unsupervised groups datasets

parametric and nonparametric say how the learning is stored and by extension how the model learns patterns

parametric - fixed number of parameters - trial and error

nonparametric - infinite (determined by the data) - counts

how does this apply to fitting the square peg in the right hole?
nonparametric counts the number of corners
parametric trial and error


supervised parametric (pg15 predicting will red sox win) can be thought of as a search problem, want to get all the knobs in the best position
	- predict, compare to truth, learn from pattern

unsupervised parametric-
looks like decide number of groups
 adjust parameters  so the datapoint fits the right group

nonparametric 
oversimplified, counting
3 lights and in each combo of lights on/off has stop or go


parameters, a generic term, set of numbers used to model a pattern

present enough (what you would show a human to make the same prediction) to the network

"number of columns in the input set" or "number of datapoints processing at once"



nonparametric counts new things or adds new knobs when it sees something new





###########


What is a NN? one or more weights we multiple by the input to make a prediction

the interface?

input as information, weight variables as knowledge, outputs a prediction

think of the weight value as sensitivity

when you take two vectors of same length and do a similar operation on both, it's called an elementwise operation


dot product gives a notion of similarity between two vectors

Dot product properties
[1,0,1,0]
[0,1,1,0]
if it has value or doesn't, think of that as logical AND

[.5,0,1,0]
[-1,1,1,0]
think of negative as logical NOT

so think of these as:
(a[0] AND b[0]) OR (a[1] AND b[1]) OR (a[2] AND b[2]) OR (a[3] AND b[3])


##########

pg around 39 --> 
explain how to use 1 datapoint to predict multiple things
use multiple datapoints to predict 1 thing
use multiple datapoints to predict multiple things 

matrix multiplication
take input and dor product it with each vector of the weight matrix to predict all the different things



reading numpy code is all about reading operations and keeping track of shape

squaring the error amplifies the high scores and reduces the smaller error
why not just take abs(ground_truth - pred)?
pg 52 why keep error positive?



Mean Squared Error
(pred - true_val) ** 2 

node delta - (pred - true_val) 

stopping
negative reversal


number_of_toes = [8.5]
win_or_lose_binary = [1] # (won!!!)

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input,weight)
error = (pred - goal_pred) ** 2

prediction = .85
(network was too low by .15)
node_delta = -.15

# how much a weight caused network to miss
weight_delta = input * delta
"scaling output node's delta with input to the weigh"t


For any input and goal_pred, the exact relationship is defined between error and weight
error = ((input * weight) - .8) ** 2

pg 61 profound


pg 64: thought experiments
change goal (goal_pred) of the model... "giving up"
change input and error to 0, see world how you want it (inception)
error calc meaningless if doesn't give a good measure of how much you missed

p65
changing weights means the function conforms to the patterns of the data
since the rest of the function is not changing (besides weight) force function to correctly model a pattern in the data; only allowed to model how the function predicts,

therefore you modify specific parts of the error function, in this case
error = ((input * weight) - goal_pred) ** 2
until the error is zero

you can't change input




understanding the relationship between weight and error
if you do, know how to adjust weight to reduce error
== how changing one var changes the other var == sensitivity(direction and amount) between the two variables

derivative == "when I change red rod, this is how much the blue rod changes"
there's always a derivative between 2 variables

With derivatives, you can pick 2 vars in any formula and know how they interact

neural network, a bunch of weights used to compute error

can compute relationship between weight and error
with that we can change each weight in NN to reduce weight to zero

derivative is the sensitivity between two variables






pg 70 (having trouble understanding it)

weight = 0.0
goal_pred = 0.8
input = 1.1

for iteration in range(4):
    pred = input * weight
    print(pred)
    error = (pred - goal_pred) ** 2
    
    delta = pred - goal_pred
    weight_delta = delta * input
    weight = weight - weight_delta

    print("Error:" + str(error) + " Prediction:" + str(pred))


in this ex, we're relating weight and error, to get weight closer
(pred - goal_pred) * input
think of it like we're doing 2 different things



pg 83
delta is how much you want node's output to be different
how to transform delta to weight_ delta (multiply by the input)


each input has a common delta and a unique input (weight_delta = input * delta )

weight - error curves represent how input contributes to error 

89
if you freeze one weight, train without it (weight_delta[i] = 0), NN learns data without it
error for the froze weight still goes to zero
if you unfreeze well error is already low so it won't use that weight much  
In effect these weight - error curves are 2D slices of a 4D graph ( in this case 3 weight, and error dimension)
curvature of the error plane is decided by the training data




pg 95
What does it mean to modify a bunch of weights to learn a pattern in aggregate


p 97
a dot product is a loose measurement of similarity between 2 vectors

pg 101
learned pattern by observing all the datapoints and searching for correlation


























Pro Git
Grokking DL
squadx_exp memorize current tasks, high level understand of everything else
Cooking Science

If on a win

good at job
DL researcher can read implement a paper in < 1 day,
core contributor in OpenMined
wit, worldlyness from internalizing breathing books
storyteller stage presence
flient in french
art of french cooking



write some, a goal is erudition, we'll start by defining it
if on a winters night (immerse and finish this beautiful novel)
Branching chapter of the pro git

Grokking DL
