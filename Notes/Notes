DL is a subset of ML

what are 3 primary domains in which DL used/researched?
CV, NLP, speech recognition



ML - not explicitly programmed
ML algorithms observe a pattern and attempt to directly or indirectly imitate the task

What do directly or indirectly  correspond to?
supervised and nonsupervised learning

supervised - the direct imitation of a pattern between two datasets
take input dataset and output output dataset

transforming what you know into what you want to know



machine learning has
supervised and unsupervised
parametric and nonparametric

supervised learning transforms datasets
unsupervised groups datasets

parametric and nonparametric say how the learning is stored and by extension how the model learns patterns

parametric - fixed number of parameters - trial and error

nonparametric - infinite (determined by the data) - counts

how does this apply to fitting the square peg in the right hole?
nonparametric counts the number of corners
parametric trial and error


supervised parametric (pg15 predicting will red sox win) can be thought of as a search problem, want to get all the knobs in the best position
	- predict, compare to truth, learn from pattern

unsupervised parametric-
looks like decide number of groups
 adjust parameters  so the datapoint fits the right group

nonparametric 
oversimplified, counting
3 lights and in each combo of lights on/off has stop or go


parameters, a generic term, set of numbers used to model a pattern

present enough (what you would show a human to make the same prediction) to the network

"number of columns in the input set" or "number of datapoints processing at once"



nonparametric counts new things or adds new knobs when it sees something new





###########


What is a NN? one or more weights we multiple by the input to make a prediction

the interface?

input as information, weight variables as knowledge, outputs a prediction

think of the weight value as sensitivity

when you take two vectors of same length and do a similar operation on both, it's called an elementwise operation


dot product gives a notion of similarity between two vectors

Dot product properties
[1,0,1,0]
[0,1,1,0]
if it has value or doesn't, think of that as logical AND

[.5,0,1,0]
[-1,1,1,0]
think of negative as logical NOT

so think of these as:
(a[0] AND b[0]) OR (a[1] AND b[1]) OR (a[2] AND b[2]) OR (a[3] AND b[3])


##########

pg around 39 --> 
explain how to use 1 datapoint to predict multiple things
use multiple datapoints to predict 1 thing
use multiple datapoints to predict multiple things 

matrix multiplication
take input and dor product it with each vector of the weight matrix to predict all the different things



reading numpy code is all about reading operations and keeping track of shape

squaring the error amplifies the high scores and reduces the smaller error
why not just take abs(ground_truth - pred)?
pg 52 why keep error positive?



Mean Squared Error
(pred - true_val) ** 2 

node delta - (pred - true_val) 

stopping
negative reversal


number_of_toes = [8.5]
win_or_lose_binary = [1] # (won!!!)

input = number_of_toes[0]
goal_pred = win_or_lose_binary[0]

pred = neural_network(input,weight)
error = (pred - goal_pred) ** 2

prediction = .85
(network was too low by .15)
node_delta = -.15

# how much a weight caused network to miss
weight_delta = input * delta
"scaling output node's delta with input to the weigh"t


For any input and goal_pred, the exact relationship is defined between error and weight
error = ((input * weight) - .8) ** 2

pg 61 profound


pg 64: thought experiments
change goal (goal_pred) of the model... "giving up"
change input and error to 0, see world how you want it (inception)
error calc meaningless if doesn't give a good measure of how much you missed

p65
changing weights means the function conforms to the patterns of the data
since the rest of the function is not changing (besides weight) force function to correctly model a pattern in the data; only allowed to model how the function predicts,

therefore you modify specific parts of the error function, in this case
error = ((input * weight) - goal_pred) ** 2
until the error is zero

you can't change input




understanding the relationship between weight and error
if you do, know how to adjust weight to reduce error
== how changing one var changes the other var == sensitivity(direction and amount) between the two variables

derivative == "when I change red rod, this is how much the blue rod changes"
there's always a derivative between 2 variables

With derivatives, you can pick 2 vars in any formula and know how they interact

neural network, a bunch of weights used to compute error

can compute relationship between weight and error
with that we can change each weight in NN to reduce weight to zero

derivative is the sensitivity between two variables






pg 70 (having trouble understanding it)

weight = 0.0
goal_pred = 0.8
input = 1.1

for iteration in range(4):
    pred = input * weight
    print(pred)
    error = (pred - goal_pred) ** 2
    
    delta = pred - goal_pred
    weight_delta = delta * input
    weight = weight - weight_delta

    print("Error:" + str(error) + " Prediction:" + str(pred))


in this ex, we're relating weight and error, to get weight closer
(pred - goal_pred) * input
think of it like we're doing 2 different things



pg 83
delta is how much you want node's output to be different
how to transform delta to weight_ delta (multiply by the input)


each input has a common delta and a unique input (weight_delta = input * delta )

weight - error curves represent how input contributes to error 

89
if you freeze one weight, train without it (weight_delta[i] = 0), NN learns data without it
error for the froze weight still goes to zero
if you unfreeze well error is already low so it won't use that weight much  
In effect these weight - error curves are 2D slices of a 4D graph ( in this case 3 weight, and error dimension)
curvature of the error plane is decided by the training data




pg 95
What does it mean to modify a bunch of weights to learn a pattern in aggregate


p 97
a dot product is a loose measurement of similarity between 2 vectors

pg 101
learned pattern by observing all the datapoints and searching for correlation


pg 103
matrix structure
recorded example get a row, each state being recorded is a single column (on-off state for each light)


can have [.9 0 1]
can have [18 0 20]
underlying pattern isn't the same as the matrix, it's a property of the matrix
the pattern is what each matrix is expressing

input data pattern is what you want NN to transform to the output data pattern

convert stop-walk outputs to 0, 1
lossless representation because we can convert back and forth



pg 106 
goal of NN --> take the streetlights (input dataset) and transform it to a walk_vs_stop output dataset



pg 109
stochastic gradient descent - updates weights 1 example at a time
(full) gradient descent - updates weights weights 1 dataset at a time (network calculates an average changing weights only when it computes the full average)
batch gradient descent - updates weights after k examples

if NN has 3 weights .01, 1.0, -.0001, it identified correlation of middle weight and result
each node trying to correctly predict output given the input
cross communication is when all 3 weights share same error measure
Error attribution: Given a shared error, NN needs to  figure out which weights contributed to error (seems like it's an after effect of 
weight_new = weight_prev - alpha * input * (weight * input - goal)

input_vec = [1, 0, 1] ===> middle weight has zero effect on the prediction 

suppose
input = [1, 0, 1] goal = [0]
0 is irrelevant therefore weights associated with 1's have to be low for model to predict correctly to drive the weight to zero

pg 111
"weight pressure table"

rewarding correlation with pressure on its weighttowards 1 and penalizing decorrelation with pressure toward zero

Remember
learning rewards correlation with higher weights
learning finds correlation between two datasets


pg 113
If a combination of weight[i] * input[i] perfectly classifys dataset, model stops learning, if it does this without giving heaviest weights to best inputs NN stops learning | overfitting is ML's greatest enemy

pg 113
greatest challenge in DL is convincing NN to generalize instead of memorize

pg 115
Regularization is advantageous because if a weight has equal pressure upward and downward it's useless; basically says only weights with strong correlation stay on

(on the stoplights example ch 6)
Without regularization you wouldn't learn right weight is useless until left and middle find their correlations


pg 115
no correlation
Dataset is the real problem in machine learning


It was first presented that neural networks search for correlation between input and output datasets
Let's refine that definition: In reality, NN search for correlation between their input and output layers


pg 116
dataset doesn't correlate with output
solution: use the input dataset to create an intermediate dataset that does have correlation with output. It's kind of like cheating
pg 117: this network is still a function, just a bunch of weights collected together

reread pg 119-121


pg 131
predicting cats, individual pixels have nothing to do with whether or not it is a cat. Only different configurations of pixels correlate with whether there's a cat
































































goal for today is pg 229 | learn hard, review, keep going when don't wanna 
duolingo for 200 points and a pen


Pro Git
Grokking DL
squadx_exp memorize current tasks, high level understand of everything else
Cooking Science

If on a win

good at job
DL researcher can read implement a paper in < 1 day,
core contributor in OpenMined
wit, worldlyness from internalizing breathing books
storyteller stage presence
flient in french
art of french cooking



write some, a goal is erudition, we'll start by defining it
if on a winters night (immerse and finish this beautiful novel)
Branching chapter of the pro git

Grokking DL
